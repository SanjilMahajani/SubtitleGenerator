{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjilMahajani/SubtitleGenerator/blob/main/Video_Subtitle_creator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install ffmpeg-python\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "\n"
      ],
      "metadata": {
        "id": "Ln2aXuKb68P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ffmpeg\n",
        "import whisper\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available and select the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the Whisper model on the selected device\n",
        "model = whisper.load_model(\"medium\", device=device)  # Use 'base', 'small', 'medium', or 'large' based on your requirement\n"
      ],
      "metadata": {
        "id": "czbKpKHg83s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import whisper\n",
        "import ffmpeg\n",
        "\n",
        "# Check if GPU is available and select the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Function to extract audio from video\n",
        "def extract_audio_from_video(video_path, audio_path=\"your location.wav\"):\n",
        "    try:\n",
        "        # Use ffmpeg to extract audio\n",
        "        ffmpeg.input(video_path).output(audio_path).run(overwrite_output=True, quiet=True)\n",
        "        print(f\"Audio extracted to {audio_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting audio: {e}\")\n",
        "\n",
        "# Function to convert seconds to SRT time format\n",
        "def convert_seconds_to_srt_time(seconds):\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    seconds = int(seconds % 60)\n",
        "    milliseconds = int((seconds % 1) * 1000)\n",
        "    return f\"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}\"\n",
        "\n",
        "# Function to transcribe audio to English text using Whisper with timestamps\n",
        "def transcribe_audio_to_english_with_timestamps(model, audio_path):\n",
        "    try:\n",
        "        # Transcribe audio using Whisper with translation to English and timestamps\n",
        "        result = model.transcribe(audio_path, task=\"translate\", condition_on_previous_text=False)\n",
        "        segments = result['segments']\n",
        "        return segments\n",
        "    except Exception as e:\n",
        "        print(f\"Error transcribing audio: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to create SRT file from Whisper transcription segments\n",
        "def create_srt_from_segments(segments, output_srt_file):\n",
        "    with open(output_srt_file, 'w') as srt_file:\n",
        "        for index, segment in enumerate(segments):\n",
        "            start_time = segment['start']\n",
        "            end_time = segment['end']\n",
        "            text = segment['text']\n",
        "            start_time_str = convert_seconds_to_srt_time(start_time)\n",
        "            end_time_str = convert_seconds_to_srt_time(end_time)\n",
        "            srt_file.write(f\"{index + 1}\\n\")\n",
        "            srt_file.write(f\"{start_time_str} --> {end_time_str}\\n\")\n",
        "            srt_file.write(f\"{text.strip()}\\n\\n\")\n",
        "\n",
        "    print(f\"Subtitle file '{output_srt_file}' created successfully.\")\n",
        "\n",
        "# Main function to convert video to English subtitles\n",
        "def video_to_english_subtitles(video_path):\n",
        "    audio_path = \"your location.wav\"\n",
        "    extract_audio_from_video(video_path, audio_path)\n",
        "\n",
        "    segments = transcribe_audio_to_english_with_timestamps(model, audio_path)\n",
        "    if segments:\n",
        "        create_srt_from_segments(segments, \"your location.srt\")\n",
        "    else:\n",
        "        print(\"Audio transcription and translation failed.\")\n",
        "\n",
        "\n",
        "video_path = \"your location.mp4\"  # Replace with your video file path\n",
        "video_to_english_subtitles(video_path)\n"
      ],
      "metadata": {
        "id": "EgMSWOj9idfR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}